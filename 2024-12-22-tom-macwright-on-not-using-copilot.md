---
layout: post
title: "Tom MacWright on not using Copilot"
summary: "A reminder about human nature in the AI loop"
description: " "
---

From Tom MacWright's blog [post](https://macwright.com/2024/11/20/not-using-copilot):
> So, in summary: maybe people shy away from copilots because they're tired of complexity, they're tired of accelerating productivity without improving hours, they're afraid of forgetting rote skills and basic knowledge, and they want to feel like writers, not managers.
>
> Maybe some or none of these things are true - they're emotional responses and gut feelings based on predictions - but they matter nonetheless.

I agree that these things are true and that they matter, and would add two thoughts:
- By nature, generative output is not guaranteed to be correct. As the user/developer/writer, I am liable for the correctness of that output. This means my tolerances for uncertainty and risk (of loss of reputation, work quality, etc.) are now nontrivial inputs in my project. Why should I deal with anxiety over randomness when I can make my own mistakes _and_ learn from them?
- Relying on assistants lowers the bar for "good enough" design. While I write code, I am actively refining what I want and making better design choices. When reviewing generated code, I can't do much of this because I am busy validating semantics the LLM's interpretation of my initial design without critical self-examination. Maybe these are growing pains of generative AI software, I just haven't figured out my workflow yet.

I want generative AI to help me with my job so I can play outside. The tradeoffs of the current generation aren't worth the products.
